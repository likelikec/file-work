import os
import requests
import json
import pandas as pd
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from typing import List, Tuple

# é…ç½® API å¯†é’¥å’Œ URL
API_KEY = "sk-BNBbQFIP7c4E04c33f86T3BlbKFJ4Ae550543E8b4e42b4b3"
API_URL = "https://cn2us02.opapi.win/v1/chat/completions"

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
}


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®ç±»"""
    name: str
    provider: str
    max_tokens: int = 2048
    temperature: float = 0.7
    supports_json_mode: bool = False


# æ”¯æŒçš„æ¨¡å‹é…ç½®
SUPPORTED_MODELS = {
    # OpenAIæ¨¡å‹
    "gpt-3.5-turbo": ModelConfig("gpt-3.5-turbo", "openai", 2048, 0.7, True),
    "gpt-4o-mini-2024-07-18": ModelConfig("gpt-4o-mini-2024-07-18", "openai", 12288, 0.5, True),

    # Anthropic Claudeæ¨¡å‹
    # "claude-3.7-sonnet": ModelConfig("claude-3-5-sonnet-20241022", "anthropic", 4096, 0.7, False),

    # Google Geminiæ¨¡å‹
    "gemini-2.5-flash-lite-preview-06-17": ModelConfig("gemini-2.5-flash-lite-preview-06-17", "google", 12288, 0.5, False),

    # Qwenæ¨¡å‹
    # "qwen-turbo": ModelConfig("qwen-turbo", "qwen", 2048, 0.7, False),
    # "qwen-plus": ModelConfig("qwen-plus", "qwen", 4096, 0.7, False),
    # "qwen-max": ModelConfig("qwen-max", "qwen", 4096, 0.7, False),
    # "qwen2-72b": ModelConfig("qwen2-72b-instruct", "qwen", 4096, 0.7, False),

    # å…¶ä»–æ¨¡å‹å¯ä»¥ç»§ç»­æ·»åŠ 
}


def get_available_models() -> List[str]:
    """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    return list(SUPPORTED_MODELS.keys())


def print_available_models():
    """æ‰“å°å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
    print("å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for provider_group in ["openai", "anthropic", "google", "qwen"]:
        models = [name for name, config in SUPPORTED_MODELS.items() if config.provider == provider_group]
        if models:
            provider_name = {
                "openai": "OpenAI",
                "anthropic": "Anthropic Claude",
                "google": "Google Gemini",
                "qwen": "Qwen"
            }.get(provider_group, provider_group)
            print(f"\n{provider_name}:")
            for model in models:
                print(f"  - {model}")


# è·å–é¡¹ç›®ç›®å½•ä¸‹æ‰€æœ‰ .java æ–‡ä»¶
def get_java_files(project_dir):
    java_files = []
    for root, dirs, files in os.walk(project_dir):
        for file in files:
            if file.endswith(".java"):
                java_files.append(os.path.join(root, file))
    return java_files


# ä»æ–‡ä»¶è·¯å¾„æå–ç±»å
def get_class_name(file_path):
    return os.path.splitext(os.path.basename(file_path))[0]


# è¯»å–æ–‡ä»¶å†…å®¹
def read_file_content(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()


# æ„é€ æç¤º
def construct_prompt(class_name: str, class_code: str) -> str:
    prompt = f"""
Here is a Java class named {class_name}:

{class_code}
You are a senior professor of software engineering. Please classify each class based on your professional knowledge and the following 25 indicators that describe code characteristics. Read the code, combine the characteristics of test case generation by Evo and LLM, calculate the 25 indicators that describe code characteristics to determine whether it is more appropriate to use LLM or Evosuite to generate test cases.

Evosuite is a tool that automatically generates Java-like test cases using evolutionary algorithms, with the goal of achieving high code coverage. LLM can generate test cases based on its understanding of the code and behavior.

The 15 metrics are as follows:

1. **Number of Transitive Dependencies (Dcy*)** : Derived from Dcy, it refers to the number of other classes that a class directly or indirectly depends on, including the number of directly dependent classes and indirectly dependent classes, which can reflect the deep coupling state of the class in the dependency network.
2. **Number of Transitive Dependents (DPT*)** : It is an object-oriented coupling feature, referring to the number of classes that indirectly depend on the current class through the dependency chain. It is obtained by constructing a dependency graph and using a graph traversal algorithm to find all transitive dependencies and then statistically analyzing them.
3. **Cyclic Dependencies ** : It is an object-oriented coupling feature, referring to the number of other classes that a class directly or indirectly depends on, and these dependencies also directly or indirectly depend on that class. It is obtained by constructing a dependency graph, collecting direct dependencies, calculating transitive dependencies, identifying strongly connected components, and then subtracting 1 after statistics, representing the number of other classes involved in circular dependencies.
4. **Level (Level)** : It is an object-oriented theoretical complexity feature that measures the "number of levels" of the classes a class depends on. When it does not depend on other classes, its value is 0; when it does depend, its value is the maximum Level value among the dependent classes plus 1 (excluding classes that are mutually dependent or cyclically dependent).
5. Adjusted Level (Level*) : It is a theoretical complexity feature of object-oriented programming. Based on the "number of layers" of the classes that a class depends on, it considers the number of classes in circular dependencies. When it does not depend on other classes, the value is 0. When there is a dependency, the value is the maximum Level* value in the dependent class (non-circular dependency) plus the number of classes that are interdependent with or form a circular dependency with that class.
6.**Package Dependents (PDpt)** : It is the dependency feature corresponding to PDcy, referring to the number of packages that directly or indirectly depend on the current class. It is obtained by counting the number of packages that directly or indirectly depend on the class.
7.Lack of Cohesion of Methods (LCOM) : It is an object-oriented cohesion feature, referring to the degree of lack of cohesion among the methods of a class. It is obtained by constructing an inter-method relationship graph (where nodes represent methods and edges represent inter-method relationships) and calculating the number of connected components in the graph, and is related to the cohesion and responsibility of the class.
8.**Comment Lines of Code (CLOC)** : It is an extension of the concept of source code lines of code (SLOC) proposed by Lazic. It refers to the total number of lines containing comment content in the code file, calculated by strict syntax parsing, reflecting the physical density of comments in the code and its relationship with the interpretability of the code.
9. **Javadoc Lines of Code (JLOC)** : It is a refined metric of CLOC, specifically counting the number of lines of comments that comply with the Javadoc specification, measuring the density of code comments that follow the conventions of the standard API documentation, and is closely related to the completeness of the API documentation.
10. **Javadoc Field Coverage (JF)** : It is a coverage feature based on JLOC, referring to the percentage of the number of fields with Javadoc comments to the total number of fields in the class, quantifying the documentation level of fields in the class.
11. **Javadoc Method Coverage (JM)** : Derived from JLOC, it refers to the percentage of the number of methods with Javadoc annotations to the total number of methods in the class, and is closely related to the integrity of the method-level documentation.
12. **Comment Ratio (COM_RAT)** : It is a code comment feature, referring to the ratio of the number of comment lines in the code to the total number of code lines (excluding blank lines), reflecting the relative density of comments in the code. It is a classic feature for evaluating code readability.
13.**Number of Implemented Interfaces (INNER)** : Defined considering the need to describe the size of a class from the perspective of interface implementation, it refers to the total number of interfaces implemented by a class, including all different interfaces implemented directly and inherited from the parent class.
14.String Processing** : This scenario involves the creation, manipulation, parsing, formatting or transformation of strings, such as processing text data, generating reports, cleaning input, as well as methods like string splitting, concatenation, and regular expression matching.
15.*Business Logic** : This scenario mainly implements specific business rules or domain logic, which is specific to the application context, such as user permission verification, workflow engine, etc.


Please respond in the following JSON format:

{{"class_name": "{class_name}", "tool": "LLM" æˆ– "Evosuite"}}
"""
    return prompt


def construct_api_params(prompt: str, model_name: str) -> Dict[str, Any]:
    """æ ¹æ®æ¨¡å‹æ„é€ APIå‚æ•°"""
    if model_name not in SUPPORTED_MODELS:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

    config = SUPPORTED_MODELS[model_name]

    # åŸºç¡€å‚æ•°
    params = {
        "model": config.name,
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": config.max_tokens,
        "temperature": config.temperature,
        "stream": False
    }

    # å¦‚æœæ¨¡å‹æ”¯æŒJSONæ¨¡å¼ï¼Œæ·»åŠ å“åº”æ ¼å¼
    if config.supports_json_mode:
        params["response_format"] = {"type": "json_object"}

    return params


def parse_response(response_json: Dict[str, Any], model_name: str) -> Optional[Dict[str, Any]]:
    """è§£æä¸åŒæ¨¡å‹çš„å“åº”"""
    try:
        config = SUPPORTED_MODELS[model_name]

        # è·å–å“åº”å†…å®¹
        if config.provider in ["openai", "qwen"]:
            content = response_json["choices"][0]["message"]["content"]
        elif config.provider == "anthropic":
            content = response_json["content"][0]["text"] if "content" in response_json else \
            response_json["choices"][0]["message"]["content"]
        elif config.provider == "google":
            content = response_json["candidates"][0]["content"]["parts"][0][
                "text"] if "candidates" in response_json else response_json["choices"][0]["message"]["content"]
        else:
            # é»˜è®¤å¤„ç†æ–¹å¼
            content = response_json["choices"][0]["message"]["content"]

        # å°è¯•è§£æJSON
        if content.strip().startswith("{"):
            return json.loads(content)
        else:
            # å¦‚æœä¸æ˜¯JSONæ ¼å¼ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{.*?\}', content, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                print(f"æ— æ³•è§£æå“åº”å†…å®¹: {content}")
                return None

    except Exception as e:
        print(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
        return None


# è°ƒç”¨ APIï¼Œæ·»åŠ é‡è¯•æœºåˆ¶
def call_api(prompt: str, model: str = "gpt-3.5-turbo", max_retries: int = 4) -> Optional[Dict[str, Any]]:
    """è°ƒç”¨APIï¼Œæ”¯æŒå¤šç§æ¨¡å‹"""
    if model not in SUPPORTED_MODELS:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹ '{model}'")
        print_available_models()
        return None

    for attempt in range(max_retries):
        try:
            params = construct_api_params(prompt, model)
            print(f"[DEBUG] æ­£åœ¨ä½¿ç”¨æ¨¡å‹ {model} å‘é€è¯·æ±‚ (å°è¯• {attempt + 1}/{max_retries})...")

            response = requests.post(
                API_URL,
                headers=HEADERS,
                json=params,
                timeout=30
            )
            response.raise_for_status()  # æ£€æŸ¥ HTTP çŠ¶æ€ç 
            print("[DEBUG] è¯·æ±‚æˆåŠŸï¼")

            res_json = response.json()
            tool_dict = parse_response(res_json, model)

            if tool_dict:
                return tool_dict
            else:
                print(f"è§£æå“åº”å¤±è´¥ï¼Œå°è¯•é‡è¯•...")

        except requests.exceptions.RequestException as e:
            print(f"ç½‘ç»œè¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
        except Exception as e:
            print(f"API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")

        if attempt < max_retries - 1:
            time.sleep(2)  # ç­‰å¾… 2 ç§’åé‡è¯•

    print(f"æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œè·³è¿‡æ­¤æ¬¡è¯·æ±‚")
    return None


# å¤„ç†å•ä¸ªé¡¹ç›®
def process_project(project_dir: str, model: str = "gpt-3.5-turbo") -> Tuple[List[List[str]], str]:
    java_files = get_java_files(project_dir)
    results = []

    print(f"æ‰¾åˆ° {len(java_files)} ä¸ªJavaæ–‡ä»¶")

    for i, file_path in enumerate(java_files, 1):
        class_name = get_class_name(file_path)
        print(f"å¤„ç†æ–‡ä»¶ {i}/{len(java_files)}: {class_name}")

        try:
            class_code = read_file_content(file_path)
            prompt = construct_prompt(class_name, class_code)
            response = call_api(prompt, model)

            if response:
                try:
                    results.append([response["class_name"], response["tool"]])
                    print(f"  âœ“ {class_name} -> {response['tool']}")
                except KeyError as e:
                    print(f"  âœ— è§£æ {class_name} çš„å“åº”æ—¶ç¼ºå°‘å­—æ®µ: {e}")
                except Exception as e:
                    print(f"  âœ— è§£æ {class_name} çš„å“åº”æ—¶å‡ºé”™: {e}")
            else:
                print(f"  âœ— è·³è¿‡ {class_name}ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶")
        except Exception as e:
            print(f"  âœ— å¤„ç†æ–‡ä»¶ {class_name} æ—¶å‡ºé”™: {e}")

    # è¿”å›ç»“æœå’Œé¡¹ç›®å
    project_name = os.path.basename(os.path.normpath(project_dir))
    return results, project_name


def test_model_call(model_name: str = "gpt-3.5-turbo") -> bool:
    """æµ‹è¯•æ¨¡å‹è°ƒç”¨æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹è°ƒç”¨: {model_name}")
    print(f"{'=' * 50}")

    # éªŒè¯æ¨¡å‹æ˜¯å¦æ”¯æŒ
    if model_name not in SUPPORTED_MODELS:
        print(f"âŒ é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹ '{model_name}'")
        print_available_models()
        return False

    # æ„é€ æµ‹è¯•æç¤º
    test_class_name = "TestClass"
    test_class_code = """
public class TestClass {
    private int value;

    public TestClass(int value) {
        this.value = value;
    }

    public int getValue() {
        return value;
    }

    public void setValue(int value) {
        this.value = value;
    }

    public int add(int num) {
        return value + num;
    }
}
"""

    test_prompt = construct_prompt(test_class_name, test_class_code)

    print(f"ğŸ“¤ å‘é€æµ‹è¯•è¯·æ±‚...")
    print(f"   æ¨¡å‹: {model_name}")
    print(f"   é…ç½®: {SUPPORTED_MODELS[model_name]}")

    try:
        # è°ƒç”¨API
        response = call_api(test_prompt, model_name, max_retries=2)

        if response is None:
            print(f"âŒ æµ‹è¯•å¤±è´¥: APIè°ƒç”¨è¿”å›None")
            return False

        # éªŒè¯å“åº”æ ¼å¼
        if not isinstance(response, dict):
            print(f"âŒ æµ‹è¯•å¤±è´¥: å“åº”ä¸æ˜¯å­—å…¸æ ¼å¼")
            return False

        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if "class_name" not in response:
            print(f"âŒ æµ‹è¯•å¤±è´¥: å“åº”ç¼ºå°‘ 'class_name' å­—æ®µ")
            print(f"   å®é™…å“åº”: {response}")
            return False

        if "tool" not in response:
            print(f"âŒ æµ‹è¯•å¤±è´¥: å“åº”ç¼ºå°‘ 'tool' å­—æ®µ")
            print(f"   å®é™…å“åº”: {response}")
            return False

        # éªŒè¯å­—æ®µå€¼
        if response["class_name"] != test_class_name:
            print(f"âš ï¸  è­¦å‘Š: class_nameä¸åŒ¹é…")
            print(f"   æœŸæœ›: {test_class_name}")
            print(f"   å®é™…: {response['class_name']}")

        if response["tool"] not in ["LLM", "Evosuite"]:
            print(f"âš ï¸  è­¦å‘Š: toolå€¼ä¸åœ¨æœŸæœ›èŒƒå›´å†…")
            print(f"   æœŸæœ›: 'LLM' æˆ– 'Evosuite'")
            print(f"   å®é™…: {response['tool']}")

        # æµ‹è¯•æˆåŠŸ
        print(f"âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"   ğŸ“‹ å“åº”å†…å®¹:")
        print(f"      ç±»å: {response['class_name']}")
        print(f"      æ¨èå·¥å…·: {response['tool']}")

        return True

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False


def run_model_tests(models_to_test: List[str] = None) -> Dict[str, bool]:
    """è¿è¡Œå¤šä¸ªæ¨¡å‹çš„æµ‹è¯•"""
    if models_to_test is None:
        # è‡ªåŠ¨ä½¿ç”¨SUPPORTED_MODELSä¸­æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        models_to_test = list(SUPPORTED_MODELS.keys())
        print(f"ğŸ“‹ è‡ªåŠ¨æ£€æµ‹åˆ°ä»¥ä¸‹å¯ç”¨æ¨¡å‹:")
        for model in models_to_test:
            print(f"   - {model}")

    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡æµ‹è¯•æ¨¡å‹...")
    print(f"   æµ‹è¯•æ¨¡å‹æ•°é‡: {len(models_to_test)}")

    results = {}
    success_count = 0

    for i, model in enumerate(models_to_test, 1):
        print(f"\nğŸ“Š è¿›åº¦: {i}/{len(models_to_test)}")

        if model not in SUPPORTED_MODELS:
            print(f"â­ï¸  è·³è¿‡ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
            results[model] = False
            continue

        try:
            success = test_model_call(model)
            results[model] = success
            if success:
                success_count += 1

            # æµ‹è¯•é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            if i < len(models_to_test):
                time.sleep(1)

        except KeyboardInterrupt:
            print(f"\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
            break
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ¨¡å‹ {model} æ—¶å‡ºç°å¼‚å¸¸: {e}")
            results[model] = False

    # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
    print(f"\n{'=' * 60}")
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“")
    print(f"{'=' * 60}")
    print(f"æ€»æµ‹è¯•æ•°é‡: {len(results)}")
    print(f"æˆåŠŸæ•°é‡: {success_count}")
    print(f"å¤±è´¥æ•°é‡: {len(results) - success_count}")
    print(f"æˆåŠŸç‡: {success_count / len(results) * 100:.1f}%")

    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for model, success in results.items():
        status = "âœ… æˆåŠŸ" if success else "âŒ å¤±è´¥"
        print(f"   {model:<30} {status}")

    return results


# ä¸»å‡½æ•°
def main(project_dirs: List[str], output_excel: str, model: str = "gpt-3.5-turbo", test_mode: bool = False):
    # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ï¼Œåªè¿è¡Œæµ‹è¯•
    if test_mode:
        print("ğŸ§ª è¿è¡Œæµ‹è¯•æ¨¡å¼...")
        success = test_model_call(model)
        if success:
            print(f"\nâœ… æ¨¡å‹ {model} æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
        else:
            print(f"\nâŒ æ¨¡å‹ {model} æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ï¼")
        return

    # éªŒè¯æ¨¡å‹
    if model not in SUPPORTED_MODELS:
        print(f"é”™è¯¯: ä¸æ”¯æŒçš„æ¨¡å‹ '{model}'")
        print_available_models()
        return

    # è¿è¡Œæµ‹è¯•ç¡®è®¤æ¨¡å‹å¯ç”¨
    print(f"ğŸ” é¦–å…ˆæµ‹è¯•æ¨¡å‹ {model} æ˜¯å¦å¯ç”¨...")
    if not test_model_call(model):
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œç¨‹åºç»ˆæ­¢")
        return

    print(f"\nâœ… æ¨¡å‹æµ‹è¯•é€šè¿‡ï¼Œå¼€å§‹å¤„ç†é¡¹ç›®...")
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_excel}")

    # åˆ›å»ºä¸€ä¸ª ExcelWriter å¯¹è±¡
    with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:
        # å¤„ç†æ¯ä¸ªé¡¹ç›®
        for project_dir in project_dirs:
            print(f"\n{'=' * 60}")
            print(f"å¼€å§‹å¤„ç†é¡¹ç›®: {project_dir}")
            print(f"{'=' * 60}")

            results, project_name = process_project(project_dir, model)

            # ä¿å­˜ç»“æœåˆ°å¯¹åº”çš„ sheet
            if results:
                df = pd.DataFrame(results, columns=["Class Name", "Suitable Tool"])
                df.to_excel(writer, sheet_name=project_name[:31], index=False)  # sheet åç§°ä¸èƒ½è¶…è¿‡31ä¸ªå­—ç¬¦
                print(f"\nâœ… é¡¹ç›® {project_name} çš„ç»“æœå·²ä¿å­˜åˆ° {output_excel} çš„ sheet: {project_name}")
                print(f"  å…±å¤„ç† {len(results)} ä¸ªç±»")
            else:
                print(f"\nâŒ é¡¹ç›® {project_name} æ²¡æœ‰ç»“æœå¯ä¿å­˜")


if __name__ == "__main__":
    # æ‰“å°å¯ç”¨æ¨¡å‹
    print_available_models()

    # å®šä¹‰7ä¸ªé¡¹ç›®çš„ç›®å½•
    project_dirs = [
        "E:\\unit-generate\\commons-csv\\src\\main\\java\\org\\apache\\commons\\csv",
        "E:\\unit-generate\\commons-lang\\src\\main\\java\\org\\apache\\commons\\lang3",
        "E:\\unit-generate\\google-json\\src\\main\\java\\com\\google\\gson",
        "E:\\unit-generate\\commons-cli-evo\\src\\main\\java\\org\\apache\\commons\\cli",
        "E:\\unit-generate\\ruler\\src\\main\\software\\amazon\\event\\ruler",
        "D:\\restful-demo-1\\dat\\src\\main\\java\\net\\datafaker",
        "E:\\unit-generate\\jfreechart154\\src\\main\\java\\org\\jfree"
    ]

    output_excel = "gemini-14-classification_results.xlsx"

    # å¯ä»¥è½»æ¾åˆ‡æ¢æ¨¡å‹
    model = "gemini-2.5-flash-lite-preview-06-17"  # å¯ä»¥æ”¹ä¸º: "claude-3.5-sonnet", "gemini-1.5-pro", "qwen-plus" ç­‰

    # ä½¿ç”¨é€‰é¡¹
    # 1. æµ‹è¯•å•ä¸ªæ¨¡å‹: è®¾ç½® TEST_MODE = True
    # 2. æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹: è®¾ç½® RUN_BATCH_TEST = True
    # 3. æ­£å¸¸è¿è¡Œ: ä¸¤ä¸ªéƒ½è®¾ç½®ä¸º False
    TEST_MODE = False  # è®¾ç½®ä¸º True åªè¿è¡Œæµ‹è¯•
    RUN_BATCH_TEST = False  # è®¾ç½®ä¸º True è¿è¡Œæ‰¹é‡æ¨¡å‹æµ‹è¯•

    if RUN_BATCH_TEST:
        # æ‰¹é‡æµ‹è¯•å¤šä¸ªæ¨¡å‹
        run_model_tests()  # ä¸ä¼ å‚æ•°ï¼Œè‡ªåŠ¨ä½¿ç”¨SUPPORTED_MODELSä¸­çš„æ‰€æœ‰æ¨¡å‹
    else:
        # æ­£å¸¸è¿è¡Œæˆ–å•ä¸ªæ¨¡å‹æµ‹è¯•
        main(project_dirs, output_excel, model, TEST_MODE)